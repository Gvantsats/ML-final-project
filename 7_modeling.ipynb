{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import random\n",
    "import scipy.ndimage\n",
    "import scipy.misc\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN:\n",
    "\n",
    "    def __init__(self , n_inputs):\n",
    "        self.label_names = ['უ', 'ყ', 'მ', 'შ', 'ძ', 'წ', 'ს', 'ხ', 'ლ', 'ჩ' , '-']\n",
    "        # learning info\n",
    "        self.n_iterations = 100000\n",
    "        self.l_rate = 0.001\n",
    "        # layer info\n",
    "        self.l_sizes = [n_inputs, 3 , 1]\n",
    "        self.n_layer = len(self.l_sizes)\n",
    "        # generating biases and weights on every hidden layer\n",
    "        self.biases = [np.random.randn(i, 1) for i in self.l_sizes[1:]]\n",
    "        self.weights = [np.random.randn(j, i) for i, j in zip(self.l_sizes[:-1], self.l_sizes[1:])]\n",
    "\n",
    "    # Activation function\n",
    "    def sigmoid(self, s):\n",
    "        return 1.0 / (np.exp(-s) + 1.0)\n",
    "\n",
    "    # Derivative of activation function\n",
    "    def sigmoid_der(self, s):\n",
    "        return self.sigmoid(s) * (1.0 - self.sigmoid(s))\n",
    "\n",
    "    # Forward propagation\n",
    "    def forward(self, data):\n",
    "        data = data.reshape(data.shape[0] , 1)\n",
    "        curr = data\n",
    "        for i in range(len(self.biases)):\n",
    "            bias = self.biases[i]\n",
    "            weight = self.weights[i]\n",
    "            mult = np.dot(weight , curr)\n",
    "            curr = self.sigmoid(mult + bias)\n",
    "        \n",
    "        return curr\n",
    "\n",
    "    # Backward propagation\n",
    "    def backward(self, X, y):\n",
    "        X = X.reshape(X.shape[0] , 1)\n",
    "        biases_err = [np.zeros((i, 1)) for i in self.l_sizes[1:]]\n",
    "        weights_err = [np.zeros((j, i)) for i, j in zip(self.l_sizes[:-1], self.l_sizes[1:])]\n",
    "        \n",
    "        # forward propagation while saving a and z values\n",
    "        a = [X]\n",
    "        z = []\n",
    "        for i in range(len(self.biases)):\n",
    "            bias = self.biases[i]\n",
    "            weight = self.weights[i]\n",
    "            curr = a[-1]\n",
    "            mult = np.dot(weight , curr)\n",
    "            z.append(mult + bias)\n",
    "            curr = self.sigmoid(mult + bias)\n",
    "            a.append(curr)\n",
    "\n",
    "        # backpropagation\n",
    "        loss = (a[-1] - y) * self.sigmoid_der(z[-1])\n",
    "        weights_err[-1] = np.dot(loss, a[-2].transpose())\n",
    "        biases_err[-1] = loss\n",
    "        \n",
    "        for i in range(2 , self.n_layer):\n",
    "            loss = np.dot(self.weights[-i + 1].transpose(), loss) * self.sigmoid_der(z[-i])\n",
    "            weights_err[-i] = np.dot(loss, a[-i - 1].transpose())\n",
    "            biases_err[-i] = loss\n",
    "\n",
    "        #update weights and biases\n",
    "        for i in range(len(self.biases)):\n",
    "            self.weights[i] -= self.l_rate * weights_err[i]\n",
    "            self.biases[i] -= self.l_rate * biases_err[i]\n",
    "\n",
    "    def training(self, data):\n",
    "        for i in range(self.n_iterations):\n",
    "            for j in range(len(data)):\n",
    "                X = data[j][0]\n",
    "                y = data[j][1]\n",
    "                self.backward(X , y)\n",
    "                \n",
    "    def classify(self , data):\n",
    "        ans = self.forward(data)[0]\n",
    "        res = [0] * len(ans)\n",
    "        ind = -1\n",
    "        for i in range(len(ans)):\n",
    "            if ans[i] > 0.5:\n",
    "                res[i] = 1\n",
    "                ind = i\n",
    "            else:\n",
    "                res[i] = 0\n",
    "        if (sum(res) > 1):\n",
    "            return '-'\n",
    "        return self.label_names[ind]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataObject:\n",
    "    # These are variables to prevent adding the same feature twice accidently.\n",
    "    ROTATE = False\n",
    "    SCALE = False\n",
    "    BLUR = False\n",
    "    NOISE = False\n",
    "\n",
    "    def __init__(self, image):\n",
    "        self.image_arr = image\n",
    "        self.flat_arr_len = image.shape[0] * image.shape[1]\n",
    "\n",
    "    def get_matrix(self):\n",
    "        return self.image_arr\n",
    "    \n",
    "    def get_array(self,shape=None):\n",
    "        shape = (self.flat_arr_len,1) if shape is None else shape\n",
    "        return self.image_arr.reshape(shape)\n",
    "\n",
    "    def set_parent_features(self, parent_obj):\n",
    "        self.ROTATE = parent_obj.ROTATE\n",
    "        self.SCALE = parent_obj.SCALE\n",
    "        self.BLUR = parent_obj.BLUR\n",
    "        self.NOISE = parent_obj.NOISE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingDataFrame:\n",
    "\n",
    "    data = {}\n",
    "    letters = []\n",
    "    DEFAULT_COLOR = 255.0\n",
    "\n",
    "    # If images are white on black, pass false as a first argument, please.\n",
    "    def __init__(self, black_on_white=True, root_dir=\"./data/ასოები/\", height=25, width=25):\n",
    "        self.HEIGHT = height\n",
    "        self.WIDTH = width\n",
    "        self.add_data(root_dir, black_on_white)\n",
    "\n",
    "    # Taking parent (children of root_dir) folder names as labels, they should be only 1 letter long.\n",
    "    # Data should be in labeled letter folders.\n",
    "    # If images are white on black, pass false as a second argument, please.\n",
    "    def add_data(self, root_dir, black_on_white=True):\n",
    "        for letter in listdir(root_dir):\n",
    "            if len(letter) > 1:\n",
    "                continue\n",
    "            for image_name in listdir(root_dir + letter):\n",
    "                img = cv2.imread(root_dir + letter + \"/\" + image_name, cv2.IMREAD_GRAYSCALE)\n",
    "                if img is None:\n",
    "                    print(\"wrong image path\")\n",
    "                else:\n",
    "                    if not black_on_white:\n",
    "                        img = 255 - img\n",
    "                        self.DEFAULT_COLOR = 0.0\n",
    "                    resized_img = cv2.resize(img, dsize=(self.WIDTH, self.HEIGHT), interpolation=cv2.INTER_CUBIC)\n",
    "                    if letter not in self.data:\n",
    "                        self.data[letter] = []\n",
    "                        self.letters.append(letter)\n",
    "                    self.data[letter].append(DataObject(resized_img))\n",
    "\n",
    "    # Rotate alphas are angles.\n",
    "    def add_rotate_f(self, rotate_alphas=(-25, -15, -5, 5, 15, 25)):\n",
    "        rotate_alphas = list(set(rotate_alphas))\n",
    "        rotate_alphas = [i for i in rotate_alphas if i % 360 != 0]  # removes angles which are useless\n",
    "        if len(rotate_alphas) == 0:\n",
    "            return\n",
    "        for letter in self.letters:\n",
    "            appendix = []\n",
    "            for sample in self.data[letter]:\n",
    "                if not sample.ROTATE:\n",
    "                    sample.ROTATE = True\n",
    "                    for angle in rotate_alphas:\n",
    "                        new_sample = scipy.ndimage.interpolation.rotate(sample.get_matrix(), angle,\n",
    "                                                                        mode='constant',\n",
    "                                                                        cval=self.DEFAULT_COLOR,\n",
    "                                                                        reshape=False)\n",
    "                        new_dataobject = DataObject(new_sample)\n",
    "                        new_dataobject.set_parent_features(sample)  # To prevent accidently using same feature twice.\n",
    "                        appendix.append(new_dataobject)\n",
    "            self.data[letter].extend(appendix)\n",
    "\n",
    "    # Scale alphas are pixels to add edges (then resize to original size).\n",
    "    # Warning: alphas that are bigger than 3 or smaller than -3 . passing them would cause an error.\n",
    "    def add_scale_f(self, scale_alphas=(2, -2,)):\n",
    "        scale_alphas = list(set([int(i) for i in scale_alphas]))\n",
    "        if 0 in scale_alphas:\n",
    "            scale_alphas.remove(0)\n",
    "        if len(scale_alphas) == 0:\n",
    "            return\n",
    "        for alpha in scale_alphas:\n",
    "            assert -4 <= alpha <= 4\n",
    "            if not -4 <= alpha <= 4:\n",
    "                print(str(alpha) + \" is forbidden, please pass correct scale alphas\")\n",
    "                return\n",
    "        for letter in self.letters:\n",
    "            appendix = []\n",
    "            for sample in self.data[letter]:\n",
    "                if not sample.SCALE:\n",
    "                    sample.SCALE = True\n",
    "                    for pixels in scale_alphas:\n",
    "                        if pixels > 0:\n",
    "                            new_sample = np.c_[np.full((self.HEIGHT + 2 * pixels, pixels), self.DEFAULT_COLOR),\n",
    "                                               np.r_[np.full((pixels, self.WIDTH), self.DEFAULT_COLOR),\n",
    "                                                     sample.get_matrix(),\n",
    "                                                     np.full((pixels, self.WIDTH), self.DEFAULT_COLOR)],\n",
    "                                               np.full((self.HEIGHT + 2 * pixels, pixels), self.DEFAULT_COLOR)]\n",
    "                        else:\n",
    "                            pixels *= -1\n",
    "                            new_sample = sample.get_matrix()[pixels:-pixels, pixels:-pixels]\n",
    "                        new_sample = cv2.resize(new_sample, dsize=(self.WIDTH, self.HEIGHT),\n",
    "                                                interpolation=cv2.INTER_CUBIC)\n",
    "                        new_dataobject = DataObject(new_sample)\n",
    "                        new_dataobject.set_parent_features(sample)  # To prevent accidently using same feature twice.\n",
    "                        appendix.append(new_dataobject)\n",
    "            self.data[letter].extend(appendix)\n",
    "\n",
    "    # Sigmas are values for blur coefficient. How much pixels should be interpolated to neighbour pixels.\n",
    "    # Please keep values between 0 < sigma < 1.\n",
    "    def add_blur_f(self, sigmas=(.1, .5)):\n",
    "        sigmas = list(set(sigmas))\n",
    "        sigmas = [i for i in sigmas if 0 < i < 1]  # removes values which are forbidden\n",
    "        if len(sigmas) == 0:\n",
    "            return\n",
    "        for letter in self.letters:\n",
    "            appendix = []\n",
    "            for sample in self.data[letter]:\n",
    "                if not sample.BLUR:\n",
    "                    sample.BLUR = True\n",
    "                    for sigma in sigmas:\n",
    "                        new_sample = scipy.ndimage.gaussian_filter(sample.get_matrix(), sigma=sigma)\n",
    "                        new_dataobject = DataObject(new_sample)\n",
    "                        new_dataobject.set_parent_features(sample)  # To prevent accidently using same feature twice.\n",
    "                        appendix.append(new_dataobject)\n",
    "            self.data[letter].extend(appendix)\n",
    "\n",
    "    # noise is maximum value added or decreased(max.:100), dots are how many dots are changed.\n",
    "    def add_noise_f(self, noise=20, dots=10):\n",
    "        if dots < 1 or 0 < noise < 100:\n",
    "            return\n",
    "        for letter in self.letters:\n",
    "            appendix = []\n",
    "            for sample in self.data[letter]:\n",
    "                if not sample.NOISE:\n",
    "                    sample.NOISE = True\n",
    "                    new_sample = np.copy(sample.get_matrix())\n",
    "                    for _ in range(dots):\n",
    "                        x = random.randint(0, self.WIDTH - 1)\n",
    "                        y = random.randint(0, self.HEIGHT - 1)\n",
    "                        if new_sample[y][x] > 200:\n",
    "                            noise *= -1\n",
    "                        elif new_sample[y][x] > 50:\n",
    "                            noise = random.randint(-noise, noise)\n",
    "                        new_sample[y][x] = new_sample[y][x] + noise\n",
    "                    new_dataobject = DataObject(new_sample)\n",
    "                    new_dataobject.set_parent_features(sample)  # To prevent accidently using same feature twice.\n",
    "                    appendix.append(new_dataobject)\n",
    "            self.data[letter].extend(appendix)\n",
    "\n",
    "    def get_random(self, letter):\n",
    "        return random.choice(self.data[letter])\n",
    "\n",
    "    def get_letter_list(self, letter):\n",
    "        return self.data[letter]\n",
    "\n",
    "    def get_letters(self):\n",
    "        return self.letters\n",
    "\n",
    "    def describe(self):\n",
    "        print(\"data contains \" + str(len(self.letters)) + \"letters, \")\n",
    "        total = 0\n",
    "        for letter in self.letters:\n",
    "            amount = len(self.data[letter])\n",
    "            total += amount\n",
    "            print(str(amount) + \" - \" + letter + \"'s.\")\n",
    "        print(\"\\nTOTAL: \" + str(total) + \" letters.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "all the input array dimensions except for the concatenation axis must match exactly",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-2ba9afe8d5cc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtrainingData\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_rotate_f\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtrainingData\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_scale_f\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mtrainingData\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_noise_f\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-65c61d894f2f>\u001b[0m in \u001b[0;36madd_scale_f\u001b[0;34m(self, scale_alphas)\u001b[0m\n\u001b[1;32m     76\u001b[0m                                                np.r_[np.full((pixels, self.WIDTH), self.DEFAULT_COLOR),\n\u001b[1;32m     77\u001b[0m                                                      \u001b[0msample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m                                                      np.full((pixels, self.WIDTH), self.DEFAULT_COLOR)],\n\u001b[0m\u001b[1;32m     79\u001b[0m                                                np.full((self.HEIGHT + 2 * pixels, pixels), self.DEFAULT_COLOR)]\n\u001b[1;32m     80\u001b[0m                         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/numpy/lib/index_tricks.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    333\u001b[0m                 \u001b[0mobjs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobjs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinal_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 335\u001b[0;31m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobjs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    336\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: all the input array dimensions except for the concatenation axis must match exactly"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "trainingData = TrainingDataFrame()\n",
    "\n",
    "trainingData.add_rotate_f()\n",
    "\n",
    "trainingData.add_scale_f()\n",
    "\n",
    "trainingData.add_noise_f()\n",
    "\n",
    "trainingData.add_blur_f(sigmas=(.1, .5)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = {'უ' : np.array([1 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0]), \n",
    "          'ყ' : np.array([0 , 1 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0]), \n",
    "          'მ' : np.array([0 , 0 , 1 , 0 , 0 , 0 , 0 , 0 , 0 , 0]),\n",
    "          'შ' : np.array([0 , 0 , 0 , 1 , 0 , 0 , 0 , 0 , 0 , 0]),\n",
    "          'ძ' : np.array([0 , 0 , 0 , 0 , 1 , 0 , 0 , 0 , 0 , 0]),\n",
    "          'წ' : np.array([0 , 0 , 0 , 0 , 0 , 1 , 0 , 0 , 0 , 0]),\n",
    "          'ს' : np.array([0 , 0 , 0 , 0 , 0 , 0 , 1 , 0 , 0 , 0]),\n",
    "          'ხ' : np.array([0 , 0 , 0 , 0 , 0 , 0 , 0 , 1 , 0 , 0]),\n",
    "          'ლ' : np.array([0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 1 , 0]),\n",
    "          'ჩ' : np.array([0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 1]) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_inputs = 625\n",
    "net = NN(625)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = []\n",
    "for letter in trainingData.get_letters():\n",
    "    for dataObj in trainingData.get_letter_list(letter):\n",
    "        numpy_arr = dataObj.get_array()\n",
    "        train.append((numpy_arr , labels[letter]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.training(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Newtork to File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"7_model.sav\"\n",
    "with open(filename , 'wb') as file:\n",
    "    net_info = {\n",
    "                \"biases\" : net.biases,\n",
    "                \"weights\" : net.weights,\n",
    "                }\n",
    "    pickle.dump(net_info, file, 2 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
